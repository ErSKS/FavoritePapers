**SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text (CVPR2018)**  
*Alexander Mathews, Lexing Xie, Xuming He*  
[[paper](https://arxiv.org/abs/1805.07030)]  

**Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present (CVPR2018)**  
*Xinpeng Chen, Lin Ma, Wenhao Jiang, Jian Yao, Wei Liu*  
[[paper](https://arxiv.org/abs/1803.11439)]  
[[code](https://github.com/chenxinpeng/ARNet/tree/master/image_captioning)]  

**Convolutional Image Captioning (CVPR2018)**  
*Jyoti Aneja, Aditya Deshpande, Alexander Schwing*  
[[paper](https://arxiv.org/abs/1711.09151)]  

**Nonparametric Method for Data-driven Image Captioning (ACL2014)**  
[[paper](http://www.aclweb.org/anthology/P14-2097)]  

**Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning**  
*Dianqi Li, Qiuyuan Huang, Xiaodong He, Lei Zhang, Ming-Ting Sun*  
[[paper](https://arxiv.org/abs/1804.00861)]  

**Learning to Guide Decoding for Image Captioning (AAAI2018)**  
*Wenhao Jiang, Lin Ma, Xinpeng Chen, Hanwang Zhang, Wei Liu*  
[[paper](https://arxiv.org/abs/1804.00887)]  

**Boosting Image Captioning with Attributes**  
*Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, Tao Mei*  
[[paper](https://arxiv.org/abs/1611.01646)]  

**Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training (ICCV2017)**  
*Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks, Mario Fritz, Bernt Schiele*  
[[paper](https://arxiv.org/abs/1703.10476)]  

**Recurrent Topic-Transition GAN for Visual Paragraph Generation**  
*Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, Eric P. Xing*  
[[paper](https://arxiv.org/abs/1703.07022)]  

**Show, Reward and Tell: Automatic Generation of Narrative Paragraph from Photo Stream by Adversarial Training (AAAI2018)**  
*Jing Wang, Jianlong Fu, Jinhui Tang, Zechao Li, Tao Mei*  
[[paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/AAAI2018_Jing_CameraReady_V3-1.pdf)]  

**Towards Diverse and Natural Image Descriptions via a Conditional GAN(ICCV2017)**  
*Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin*  
[[paper](https://arxiv.org/abs/1703.06029)]  
[[slide](https://www.slideshare.net/toshikisakai982/cviccv2017towards-diverse-and-natural-image-descriptions-via-a-conditional-gan)]  

**Contrastive Learning for Image Captioning**  
[[paper](https://papers.nips.cc/paper/6691-contrastive-learning-for-image-captioning)]  

**Describing Natural Images Containing Novel Objects with Knowledge Guided Assitance**  
*Aditya Mogadala, Umanga Bista, Lexing Xie, Achim Rettinger*  
[[paper](https://arxiv.org/abs/1710.06303v1)]  

**A Hierarchical Approach for Generating Descriptive Image Paragraphs(CVPR2017)**  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Krause_A_Hierarchical_Approach_CVPR_2017_paper.pdf)]  

**Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition(CVPR2017)**  
*Yufei Wang, Zhe Lin, Xiaohui Shen, Scott Cohen, Garrison W. Cottrell*  
[[paper](http://openhttp://openaccess.thecvf.com/content_cvpr_2017/papers/Krause_A_Hierarchical_Approach_CVPR_2017_paper.pdfaccess.thecvf.com/content_cvpr_2017/papers/Wang_Skeleton_Key_Image_CVPR_2017_paper.pdf)]  
[[supp](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Wang_Skeleton_Key_Image_2017_CVPR_supplemental.pdf)]  

**Top-Down Visual Saliency Guided by Captions(CVPR2017)**  
*Vasili Ramanishka, Abir Das, Jianming Zhang, Kate Saenko*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Ramanishka_Top-Down_Visual_Saliency_CVPR_2017_paper.pdf)]  
[[supp](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ramanishka_Top-Down_Visual_Saliency_2017_CVPR_supplemental.pdf)]  

**Self-Critical Sequence Training for Image Captioning(CVPR2017)**  
*Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, Vaibhava Goel*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.pdf)]  

**Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-In-The-Blank Image Captioning(CVPR2017)**  
*Qing Sun, Stefan Lee, Dhruv Batra*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Sun_Bidirectional_Beam_Search_CVPR_2017_paper.pdf)]  

**Beyond Instance-Level Image Retrieval: Leveraging Captions to Learn a Global Visual Representation for Semantic Retrieval(CVPR2017)**  
*Albert Gordo, Diane Larlus*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Gordo_Beyond_Instance-Level_Image_CVPR_2017_paper.pdf)]  

**Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects(CVPR2017)**  
*Ting Yao, Yingwei Pan, Yehao Li, Tao Mei*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yao_Incorporating_Copying_Mechanism_CVPR_2017_paper.pdf)]  

**Video Captioning With Transferred Semantic Attributes(CVPR2017)**  
*Yingwei Pan, Ting Yao, Houqiang Li, Tao Mei*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Pan_Video_Captioning_With_CVPR_2017_paper.pdf)]  

**Captioning Images With Diverse Objects(CVPR2017)**  
*Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond Mooney, Trevor Darrell, Kate Saenko*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Venugopalan_Captioning_Images_With_CVPR_2017_paper.pdf)]  
[[supp](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Venugopalan_Captioning_Images_With_2017_CVPR_supplemental.pdf)]  

**SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning(CVPR2017)**  
*Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, Tat-Seng Chua*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf)]  

**Semantic Compositional Networks for Visual Captioning(CVPR2017)**  
*Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, Li Deng*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.pdf)]  

**End-To-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering(CVPR2017)**  
*Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_End-To-End_Concept_Word_CVPR_2017_paper.pdf)]  

**StyleNet: Generating Attractive Visual Captions With Styles(CVPR2017)**  
*Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, Li Deng*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_StyleNet_Generating_Attractive_CVPR_2017_paper.pdf)]  

**Dense Captioning With Joint Inference and Visual Context(CVPR2017)**  
*Linjie Yang, Kevin Tang, Jianchao Yang, Li-Jia Li*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Dense_Captioning_With_CVPR_2017_paper.pdf)]]  

**Weakly Supervised Dense Video Captioning(CVPR2017)**  
*Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, Xiangyang Xue*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Weakly_Supervised_Dense_CVPR_2017_paper.pdf)]  
[[supp](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Shen_Weakly_Supervised_Dense_2017_CVPR_supplemental.pdf)]  

**Hierarchical Boundary-Aware Neural Encoder for Video Captioning(CVPR2017)**    
*Lorenzo Baraldi, Costantino Grana, Rita Cucchiara*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper.pdf)]  
[[supplement](http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Baraldi_Hierarchical_Boundary-Aware_Neural_2017_CVPR_supplemental.pdf)]  

**Attend to You: Personalized Image Captioning With Context Sequence Memory Networks(CVPR2017)**  
*Cesc Chunseong Park, Byeongchang Kim, Gunhee Kim*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Park_Attend_to_You_CVPR_2017_paper.pdf)]  

**Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning(CVPR2017)**  
*Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Knowing_When_to_CVPR_2017_paper.pdf)]  

**Deep Reinforcement Learning-Based Image Captioning With Embedding Reward(CVPR2017)**  
*Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, Li-Jia Li*  
[[paper](http://openaccess.thecvf.com/content_cvpr_2017/papers/Ren_Deep_Reinforcement_Learning-Based_CVPR_2017_paper.pdf)]  

**Self-critical Sequence Training for Image Captioning(CVPR2017)**  
[[paper](https://arxiv.org/pdf/1612.00563.pdf)]  

**Controlling Linguistic Style Aspects in Neural Language Generation**  
*Jessica Ficler, Yoav Goldberg*  
[[paper](https://arxiv.org/abs/1707.02633)]  

**Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data(CVPR2016)**  
[[paper](http://www.cs.utexas.edu/~ml/papers/hendricks.cvpr16.pdf)]  

**YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-shot Recognition(CVPR2013)**  
[[paper](https://www.cs.utexas.edu/~vsub/pdf/YouTube2Text_Recognizing_and_2013_ICCV_paper.pdf)]  

**Teaching Machines to Describe Images via Natural Language Feedback**  
*Huan Ling, Sanja Fidler*  
[[paper](https://arxiv.org/abs/1706.00130v2)]  

**Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner**  
*Tseng-Hung Chen, Yuan-Hong Liao, Ching-Yao Chuang, Wan-Ting Hsu, Jianlong Fu, Min Sun*  
[[paper](https://arxiv.org/abs/1705.00930)]  

**Generating Sentences from a Continuous Space**  
*Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jozefowicz, Samy Bengio*  
[[paper](https://arxiv.org/abs/1511.06349)]  

**FOIL it! Find One mismatch between Image and Language caption**  
*Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aurelie Herbelot, Moin Nabi, Enver Sangineto, Raffaella Bernardi*  
[[paper](https://arxiv.org/abs/1705.01359)]  

**Sort Story: Sorting Jumbled Images and Captions into Stories**  
*Harsh Agrawal, Arjun Chandrasekaran, Dhruv Batra, Devi Parikh, Mohit Bansal*  
[[paper](https://arxiv.org/abs/1606.07493v5)]  

**Dense-Captioning Events in Videos**  
*Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos Niebles*  
[[paper](https://arxiv.org/abs/1705.00754)]  
[[project](http://cs.stanford.edu/people/ranjaykrishna/densevid/)]  
[[ArXivTimes](https://github.com/arXivTimes/arXivTimes/issues/297)]  

**Prominent Object Detection and Recognition: A Saliency-based Pipeline**  
*Hamed R. Tavakoli, Jorma Laaksonen*  
[[paper](https://arxiv.org/abs/1704.07402)]  

**Deep Reinforcement Learning-based Image Captioning with Embedding Reward**  
*Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, Li-Jia Li*  
[[paper](https://arxiv.org/pdf/1704.03899.pdf)]  

**Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data(CVPR2016)**  
[[paper](http://www.cs.utexas.edu/~ml/papers/hendricks.cvpr16.pdf)]  

**Learning Two-Branch Neural Networks for Image-Text Matching Tasks**  
*Liwei Wang, Yin Li, Svetlana Lazebnik*  
[[paper](https://arxiv.org/abs/1704.03470)]  

#Image to Caption Generation  
**Recurrent Topic-Transition GAN for Visual Paragraph Generation**  
*Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang Gan, Eric P. Xing*  
[[paper](https://arxiv.org/abs/1703.07022v1)]  

**Zero-Shot Recognition using Dual Visual-Semantic Mapping Paths**  
*Yanan Li, Donghui Wang, Huanhang Hu, Yuetan Lin, Yueting Zhuang*  
[[paper](https://arxiv.org/abs/1703.05002v2)]  

**Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning**  
*Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher*  
[[paper](https://arxiv.org/abs/1612.01887)]  
[[arXivTimes](https://github.com/arXivTimes/arXivTimes/issues/238)]  

**Controllable Text Generation(ICML2017)**  
*Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing*  
[[paper](https://arxiv.org/abs/1703.00955)]  

**Hierarchical Boundary-Aware Neural Encoder for Video Captioning**  
*Lorenzo Baraldi, Costantino Grana, Rita Cucchiara*  
[[paper](https://arxiv.org/abs/1611.09312v2)]  

**Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations**  
*Brent Harrison, Upol Ehsan, Mark O. Riedl*  
[[paper](https://arxiv.org/abs/1702.07826)]  

**Comparative Study of CNN and RNN for Natural Language Processing**  
[[paper](https://arxiv.org/pdf/1702.01923v1.pdf)]  

**Phrase Localization and Visual Relationship Detection with Comprehensive Linguistic Cues(ICCV2017)**  
*Bryan A. Plummer, Arun Mallya, Christopher M. Cervantes, Julia Hockenmaier, Svetlana Lazebnik*  
[[paper](https://arxiv.org/abs/1611.06641v2)]  

**Reference Based LSTM for Image Captioning(AAAI2017)**  
[[paper](Reference Based LSTM for Image Captioning)]  

**Text-guided Attention Model for Image Captioning(AAAI2017)**  
*Jonghwan Mun, Minsu Cho, Bohyung Han*  
[[paper](https://arxiv.org/abs/1612.03557)]  

**Attention Correctness: Machine Perception vs Human Annotations in Neural Image Captioning(AAAI2017)**  
[[paper]()]  

**ImageNet MPEG-7 Visual Descriptors - Technical Report**  
*Frédéric Rayar*  
[[paper](https://arxiv.org/abs/1702.00187v1)]  

**Learning to Decode for Future Success**  
[[paper](https://arxiv.org/abs/1701.06549)]  
not image captioning, but it may be useful for?

**Incorporating Global Visual Features into Attention-Based Neural Machine Translation**  
[[paper](https://arxiv.org/abs/1701.06521)]  

牛久先生bookmark  
[[link](http://www.ai-gakkai.or.jp/my-bookmark_vol32-no1/?utm_campaign=whats-new&utm_medium=twitter&utm_source=twitter)]  

**Image-Text Multi-Modal Representation Learning by Adversarial Backpropagation**  
*Gwangbeen Park, Woobin Im*  
[[paper](https://arxiv.org/abs/1612.08354)]  


**Visual Storytelling(NAACL2016)**  
[[paper](http://m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1147.pdf)]  

**Stating the Obvious: Extracting Visual Common Sense Knowledge(NAACL2016)**  
[[paper](http://m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1023.pdf)]  

**Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings(NAACL2016)**  
[[paper](http://m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1022.pdf)]  

**Black Holes and White Rabbits:Metaphor Identification with Visual Features(NAACL2016bestlong)**  
[[paper](http://m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1020.pdf)]  



**Rich Image Captioning in the Wild(CVPR2016workshop)**  
*Rich Image Captioning in the Wild*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w12/papers/Tran_Rich_Image_Captioning_CVPR_2016_paper.pdf)]  

**MSR-VTT: A Large Video Description Dataset for Bridging Video and Language(CVPR2016)**  
*Jun Xu , Tao Mei , Ting Yao and Yong Rui*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf)]  

**Jointly Modeling Embedding and Translation to Bridge Video and Language(CVPR2016)**  
*Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and Yong Rui*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf)]  

**Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks(CVPR2016)**  
*Haonan Yu Jiang Wang Zhiheng Huang Yi Yang Wei Xu*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.pdf)]  

**Unsupervised Learning from Narrated Instruction Videos(CVPR2016)**  
*Jean-Baptiste Alayrac Piotr Bojanowski Nishant Agrawal Josef Sivic*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Alayrac_Unsupervised_Learning_From_CVPR_2016_paper.pdf)]  

**Movie Description**  
*Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, Bernt Schiele*  
[[paper](http://arxiv.org/abs/1605.03705)]  

**Sequence to Sequence -- Video to Text**  
*Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko*  
[code](https://github.com/jazzsaxmafia/video_to_sequence)]  

**Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics**  
[[paper](http://www.jair.org/media/3994/live-3994-7274-jair.pdf)]  

**Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures**(arXiv)  
[[paper](http://arxiv.org/pdf/1601.03896.pdf)]  

**Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books** (Arxiv,2015)  
*Yukun Zhu@1, Ryan Kiros@1, Richard Zemel@1, Ruslan Salakhutdinov@1, Raquel Urtasun@1, Antonio Torralba@2, Sanja Fidler@1* (@1:University of Toronto,@2:Massachusetts Institute of Technology)   
[[paper](http://arxiv.org/abs/1506.06724)]
[[code](https://github.com/ryankiros/neural-storyteller)]
[[data](http://www.cs.toronto.edu/~mbweb/)]  

**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**  
*Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio*  
[[paper](http://jmlr.org/proceedings/papers/v37/xuc15.pdf)]  
[[supplementary](http://jmlr.org/proceedings/papers/v37/xuc15-supp.pdf)]  
[[code](https://github.com/kelvinxu/arctic-captions)]  
[[video introduction](https://www.youtube.com/watch?v=kLWRKr4PT_E)]  
[[slide](http://www.slideshare.net/eunjileee/show-attend-and-tell-neural-image-caption-generation-with-visual-attention)]  
[[chainer](https://github.com/apple2373/chainer_caption_generation)]  

**Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models** (TACL, 2015)  
*Ryan Kiros, Ruslan Salakhutdinov, Richard Zemel*  
[[paper](http://arxiv.org/abs/1411.2539)]  
[[demo](http://deeplearning.cs.toronto.edu/i2thttp://deeplearning.cs.toronto.edu/i2t)]  
[[code](https://github.com/ryankiros/visual-semantic-embedding)]  

**Zero-Shot Learning by Convex Combination of Semantic Embeddings**(ICLR2014)    
[[paper](https://arxiv.org/pdf/1312.5650.pdf)]    

**DeViSE: A Deep Visual-Semantic Embedding Model**(NIPS2013)  
[[paper](https://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model)]  
[[slide(ja)(http://www.slideshare.net/beam2d/nips2013-devise)]  

##Survey
[[link](http://www.slideshare.net/metaps_JP/deep-learning-50383383)]  
[[link2](http://www.slideshare.net/YoshitakaUshiku/ss-57148161)]  
**A Survey of Current Datasets for Vision and Language Research**  
[[paper](http://cs.rochester.edu/~nasrinm/files/Papers/Survey-Vision-and-Language.pdf)]  

##generation from root word  
**A Deep Learning Approach for Arabic Caption Generation using Roots-Words(AAAI2017)**  

